
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Q2</title><meta name="generator" content="MATLAB 9.9"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2021-10-02"><meta name="DC.source" content="Q2.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">a)</a></li></ul></div><pre class="codeinput">clear; clc; close <span class="string">all</span>;
</pre><h2 id="2">a)</h2><pre class="codeinput">load(<span class="string">'regress1.mat'</span>)

figure(1)
plot(x, y, <span class="string">'r*'</span>, <span class="string">'MarkerSize'</span>, 6, <span class="string">'LineWidth'</span>, 2)
xlabel(<span class="string">'x'</span>)
ylabel(<span class="string">'y'</span>)
title(<span class="string">'Polynomial regression orders 0 to 5'</span>)
hold <span class="string">on</span>;
orders = 0:5;
X = [];
Colors = [<span class="string">'k'</span>, <span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'m'</span>, <span class="string">'c'</span>, <span class="string">'y'</span>];
Errors = zeros(size(orders, 2), 1);
<span class="comment">%X = zeros(size(x, 1), orders + 1);</span>
<span class="keyword">for</span> i = orders
    X = [X x.^i];
    <span class="comment">%X(:, i+1) = x.^i;</span>
    [U, S, V] = svd(X);
    betaOpt = V * pinv(S) * U' * y;
    predictions = X * betaOpt;
    prediction_errors = predictions - y;
    prediction_squared_error = prediction_errors' * prediction_errors;
    Errors(i+1) = prediction_squared_error;
    plot(x, predictions, Colors(i+1), <span class="keyword">...</span>
        <span class="string">'DisplayName'</span>, [<span class="string">'poly reg order = '</span>, num2str(i), <span class="string">', sq error = '</span>, num2str(prediction_squared_error)], <span class="keyword">...</span>
        <span class="string">'LineWidth'</span>, 2)
<span class="keyword">end</span>
legend(<span class="string">'location'</span>, <span class="string">'north'</span>)

figure(2);
plot(orders, Errors, <span class="string">'r-o'</span>, <span class="string">'LineWidth'</span>, 2, <span class="string">'MarkerSize'</span>, 5)
xlabel(<span class="string">'Order of Polynomial'</span>)
ylabel(<span class="string">'Squared errors'</span>)
title(<span class="string">'Squared errors as a function of degree of polynomial'</span>)
</pre><img vspace="5" hspace="5" src="Q2_01.png" alt=""> <img vspace="5" hspace="5" src="Q2_02.png" alt=""> <p>The squared errors decrease monotonically as the order of the polynomial increases. However, this results in overfitting of the data. The goal of the regression is to model the data as closely as possible with as minimal parameters as we can. From the graph of the models, we see that the models with degree 0 and 1 are an underfit. On the other, models with degrees 4 and 5 are an overfit. Models 2 and 3 are relatively better. Model 3 has a lower error but seems to assume that the data flattens on the lower end of x, which might or might not be the case. Hence Model 2 is "best" fit in this case. Another way to look at this is to look at the datapoint that lies on the sigmoid slope of the square errors and degree. Again from this plot, we find that the "best" fit is Model 2.</p><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2020b</a><br></p></div><!--
##### SOURCE BEGIN #####
clear; clc; close all;

%% a)
load('regress1.mat')

figure(1)
plot(x, y, 'r*', 'MarkerSize', 6, 'LineWidth', 2)
xlabel('x')
ylabel('y')
title('Polynomial regression orders 0 to 5')
hold on;
orders = 0:5;
X = [];
Colors = ['k', 'b', 'g', 'm', 'c', 'y'];
Errors = zeros(size(orders, 2), 1);
%X = zeros(size(x, 1), orders + 1);
for i = orders
    X = [X x.^i];
    %X(:, i+1) = x.^i;
    [U, S, V] = svd(X);
    betaOpt = V * pinv(S) * U' * y;
    predictions = X * betaOpt;
    prediction_errors = predictions - y;
    prediction_squared_error = prediction_errors' * prediction_errors;
    Errors(i+1) = prediction_squared_error;
    plot(x, predictions, Colors(i+1), ...
        'DisplayName', ['poly reg order = ', num2str(i), ', sq error = ', num2str(prediction_squared_error)], ...
        'LineWidth', 2)
end
legend('location', 'north')

figure(2);
plot(orders, Errors, 'r-o', 'LineWidth', 2, 'MarkerSize', 5)
xlabel('Order of Polynomial')
ylabel('Squared errors')
title('Squared errors as a function of degree of polynomial')

%%
% The squared errors decrease monotonically as the order of the polynomial
% increases. However, this results in overfitting of the data. The goal of
% the regression is to model the data as closely as possible with as
% minimal parameters as we can. From the graph of the models, we see that
% the models with degree 0 and 1 are an underfit. On the other, models with
% degrees 4 and 5 are an overfit. Models 2 and 3 are relatively better.
% Model 3 has a lower error but seems to assume that the data flattens on
% the lower end of x, which might or might not be the case. Hence Model 2
% is "best" fit in this case. Another way to look at this is to look at the
% datapoint that lies on the sigmoid slope of the square errors and degree.
% Again from this plot, we find that the "best" fit is Model 2.
##### SOURCE END #####
--></body></html>