
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Lab 10: Classification, Regularization and Clustering</title><meta name="generator" content="MATLAB 9.11"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2021-12-16"><meta name="DC.source" content="lab_10_solutions.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Lab 10: Classification, Regularization and Clustering</h1><!--introduction--><p>SOLUTIONS</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Classification</a></li><li><a href="#2">Regularization</a></li><li><a href="#3">Clustering</a></li><li><a href="#4">Functions</a></li></ul></div><h2 id="1">Classification</h2><pre class="codeinput"><span class="comment">% The function boundaryViz provided at the bottom of this script takes in</span>
<span class="comment">% two sets of data and follows the procedure outlined below in order to</span>
<span class="comment">% visualize a decision boundary between the two classes:</span>
<span class="comment">%</span>
<span class="comment">% 1) Compute estimates of each groups covariance structure (the</span>
<span class="comment">%    empirically observed covariance).</span>
<span class="comment">%</span>
<span class="comment">% 2) For each point in a reasonable range, evaluate the likelihood a sample</span>
<span class="comment">%    said point came from either group, which is modeled as gaussian with</span>
<span class="comment">%    mu = mu_obs, sigma = sigma_obs</span>
<span class="comment">%</span>
<span class="comment">% 3) Use the matlab contour function to draw the decision boundary</span>
<span class="comment">%</span>
<span class="comment">% Make sure you understand (roughly) the implementation of this function</span>
<span class="comment">% and how the example below works</span>

sigma_1 = [1, -0.5;-0.5, 1];
sigma_2 = [1, 0.5; 0.5, 1];
mu_1 = [2, 1]';
mu_2 = [2, -2]';

X1 = ndRandn(mu_1, sigma_1, 500);
X2 = ndRandn(mu_2, sigma_2, 500);

 boundaryViz(X1, X2);

 <span class="comment">% 1) Quadratic Discriminant Analysis</span>
 <span class="comment">% QDA assumes that the processing underlying the data generation for each</span>
 <span class="comment">% class is its own independent Gaussian distribution, but nothing else.</span>
 <span class="comment">% That is QDA solves the most general version of the classification</span>
 <span class="comment">% problem if we assume that each class is well modeled by a gaussian (with</span>
 <span class="comment">% each class having its own mean and covariance matrix)</span>
 <span class="comment">%</span>
 <span class="comment">% Use the functions ndRandn (should be familiar) and boundaryViz to</span>
 <span class="comment">% visualize some boundary curves in the general case (where the 2 classes</span>
 <span class="comment">% have data drawn from completely unrelated normal distributions).</span>
 <span class="comment">%</span>
 <span class="comment">% Start off with the settings below and then toggle the means,</span>
 <span class="comment">% covariances, and number of points used. Try to qualitatively explain</span>
 <span class="comment">% what you observe.</span>

 <span class="comment">% try these, then generate some of your own examples</span>
sigma_1 = [1, -0.5;-0.5, 1];
sigma_2 = [1, 0.5; 0.5, 1];
sigma_3 = [0.3, 0.25; 0.25, 1];
mu_1 = [2, 1]';
mu_2 = [2, -2]';
mu_3 = [0, 0]';

n_pts = 500; <span class="comment">% vary this</span>
X1 = ndRandn(mu_1, sigma_1, n_pts);
X2 = ndRandn(mu_2, sigma_2, n_pts);
X3 = ndRandn(mu_3, sigma_3, n_pts);

<span class="comment">% ANSWER: call this many times</span>
boundaryViz(X1, X3);

 <span class="comment">% 2) Fisher Linear Discriminant</span>
 <span class="comment">% The general problem above can be constrained by assuming that</span>
 <span class="comment">% the covariance strucuture is shared between the two classes (they have</span>
 <span class="comment">% the same covariance matrix but different means). Now generate two sets</span>
 <span class="comment">% of data that fit this description and visualize the decision boundary.</span>
 <span class="comment">% As above first use the set of parameters provided below and then play</span>
 <span class="comment">% around with each (the shared covariance, mean values, and number of data</span>
 <span class="comment">% points) to get a sense for how each affects the solution.</span>

<span class="comment">% now data from all classes should have same covariance</span>
n_pts = 500; <span class="comment">% vary this</span>
sigma = sigma_1;
X1 = ndRandn(mu_1, sigma, n_pts);
X2 = ndRandn(mu_2, sigma, n_pts);
X3 = ndRandn(mu_3, sigma, n_pts);

<span class="comment">% ANSWER: call this many times</span>
boundaryViz(X1, X3);

 <span class="comment">% In class it was shown that this constraint is actually sufficient to</span>
 <span class="comment">% arrive at a closed for solution and that the optimal decision boundary</span>
 <span class="comment">% is a linear surface (a line in 2-D). How well does this match the</span>
 <span class="comment">% visualizations you generated empirically above? Why?</span>

 <span class="comment">% Answer: The decision boundary appears to be close to a plane most of the</span>
 <span class="comment">% time, but there is sometimes some noticable curve. This is because the</span>
 <span class="comment">% empirically inferred covariances from each class do not exactly meet the</span>
 <span class="comment">% assumptions for which the FLD is optimal. Indeed one of the central</span>
 <span class="comment">% difficulties of using QDA in high dimensions is that a large amount of</span>
 <span class="comment">% data is needed to accurately estimate the covariance matrix. On the</span>
 <span class="comment">% other hand, this is one of the clear benefits to using the special case</span>
 <span class="comment">% solutions: if the required assumptions are (approximately) true, you can</span>
 <span class="comment">% get a closer to optimal solution using less data than when applying the</span>
 <span class="comment">% unconstrained method.</span>


 <span class="comment">% 3) The Prototype Classifier</span>
 <span class="comment">% Finally we can restrict the problem to the special case where the</span>
 <span class="comment">% classes share a covariance matrix and that covariance matrix is a scalar</span>
 <span class="comment">% multiple of the identity matrix. Generate data that according to this</span>
 <span class="comment">% assumption and visualize the resulting decision boundaries. (First use</span>
 <span class="comment">% the provided parameters then...you know)</span>
n_pts = 500; <span class="comment">% vary this</span>
s = 1.0;
sigma = [1.0, 0; 0, 1.0];

X1 = ndRandn(mu_1, sigma, n_pts);
X2 = ndRandn(mu_2, sigma, n_pts);
X3 = ndRandn(mu_3, sigma, n_pts);

boundaryViz(X2, X3);


 <span class="comment">%  In class we saw that the MLE solution in this setting leads to a</span>
 <span class="comment">%  boundary surface that is the perpendicular bisector of the line segment</span>
 <span class="comment">%  between the class means. Does the method for computing boundary curves</span>
 <span class="comment">%  employed here seem to reliably find that solution?</span>

 <span class="comment">% Answer: Similar to above. The quality of the QDA as it pertains to</span>
 <span class="comment">% approaching the prototype classifier on this data (which we know to be</span>
 <span class="comment">% optimal) depends on the ability to accurately estimate the covariance</span>
 <span class="comment">% matrices from the data.</span>
</pre><img vspace="5" hspace="5" src="lab_10_solutions_01.png" alt=""> <img vspace="5" hspace="5" src="lab_10_solutions_02.png" alt=""> <img vspace="5" hspace="5" src="lab_10_solutions_03.png" alt=""> <img vspace="5" hspace="5" src="lab_10_solutions_04.png" alt=""> <h2 id="2">Regularization</h2><pre class="codeinput"><span class="comment">% Below is an example meant to demonstrate the syntax used by matlab's</span>
<span class="comment">% built in functions for ridge and lasso regression</span>
x = -1:0.01:1.0;
x = x';
X = [x, x.^2, x.^3, x.^4, x.^5, x.^6, x.^7, x.^8, x.^9, x.^10];
weights = randn(11, 1);
weights = [1 0 1 0 1 0 0 0 0 0]';
y = X*weights + 0.5*randn(size(x));

[B_lasso, finfo] = lasso(X, y, <span class="string">'lambda'</span>, 0.03, <span class="string">'Intercept'</span>, true);
B_lasso;
lambda = 0.01;
B_ridge = ridge(y, X, lambda);
B_ridge;

figure; hold <span class="string">on</span>;
plot(x, y);
plot(x, X*B_lasso);
plot(x, X*B_ridge);
legend(<span class="string">'Noisy Data'</span>, <span class="string">'Lasso Fit'</span>, <span class="string">'Ridge Fit'</span>);

<span class="comment">% 1) Ridge Regression</span>
<span class="comment">% We've shown previously optimizing mean square error is equivalent to</span>
<span class="comment">% solving MLE if one assumes a gaussian model for the data (with the key</span>
<span class="comment">% observation being that taking a log of a gaussian likelihood function</span>
<span class="comment">% leaves you with a squared error term). More recently we saw that adding</span>
<span class="comment">% an additive L2 penalty to the objective function is equivalent to</span>
<span class="comment">% imposing a Gaussian prior on the parameters and then performing MAP</span>
<span class="comment">% estimation.</span>
<span class="comment">%</span>
<span class="comment">% First we will tease apart what it means to "enforce some prior," by</span>
<span class="comment">% putting the model in a situation where it only has the information</span>
<span class="comment">% provided by its prior to decide on a set of parameters: fitting noise.</span>
<span class="comment">% Fill in the skeleton code below and explain the results.</span>

<span class="comment">% define regressor features (note lack of y-intercept)</span>
x = [-1:0.01:1]';
X = [x, x.^2, x.^3, x.^4, x.^5, x.^6, x.^7, x.^8, x.^9, x.^10];

<span class="comment">% define lambda</span>
lambda = 0.1;

n_expts = 1000;
running_list_of_all_coeffs = [];
<span class="keyword">for</span> ii = 1:n_expts
    <span class="comment">% make some noise to fit</span>
    y = randn(size(x));

    <span class="comment">% For the purpose of this lab, you are free to use either the closed \</span>
    <span class="comment">% form solutionfound in the slides from class or matlab's built-in</span>
    <span class="comment">% function. On the HW you should compute the solution for ridge</span>
    <span class="comment">% regression yourself.</span>
    B = 0; <span class="comment">% REPLACE THIS LINE</span>

    <span class="comment">%Answer</span>
    B = ridge(y, X, lambda);
    running_list_of_all_coeffs = [running_list_of_all_coeffs, B'];

<span class="keyword">end</span>

figure;
hist(running_list_of_all_coeffs, 200);

<span class="comment">% Does the shape of the distriubition of parameters make sense given the</span>
<span class="comment">% MAP interpretation of ridge regression?</span>

<span class="comment">% 2) LASSO</span>
<span class="comment">% We could instead choose to penalize using the L1 norm of the parameters,</span>
<span class="comment">% doing so is the basis of LASSO regression. What difference does this</span>
<span class="comment">% choice make? Consider the unit circle: all points on the circle by</span>
<span class="comment">% definition all have the same L2 norm, but the four axis intercepts have</span>
<span class="comment">% minimal L1 norms. Thus we might expect LASSO to prefer solutions where</span>
<span class="comment">% some parameters are pushed towars exactly zero.</span>

<span class="comment">% Repeat the noise fitting experiment above, but this time use Lasso and</span>
<span class="comment">% lambda = 0.001</span>

<span class="comment">% ANSWER:</span>
n_expts = 1000;
running_list_of_all_coeffs = [];
<span class="keyword">for</span> ii = 1:n_expts
    <span class="comment">% make some noise to fit</span>
    y = randn(size(x));

    <span class="comment">% For the purpose of this lab, you are free to use either the closed \</span>
    <span class="comment">% form solutionfound in the slides from class or matlab's built-in</span>
    <span class="comment">% function. On the HW you should compute the solution for ridge</span>
    <span class="comment">% regression yourself.</span>
    B = 0; <span class="comment">% REPLACE THIS LINE</span>

    <span class="comment">%Answer</span>
    B = lasso(X, y, <span class="string">'lambda'</span>, 0.001);
    running_list_of_all_coeffs = [running_list_of_all_coeffs, B'];

<span class="keyword">end</span>
figure;
hist(running_list_of_all_coeffs, 200);

<span class="comment">% Does the shape of the distriubition of parameters make sense given the</span>
<span class="comment">% MAP interpretation of LASSO?</span>

<span class="comment">% 3) Behavior with variable lambda and SNR</span>
<span class="comment">% Change between different weight settings, values for the variable sigma,</span>
<span class="comment">% and values of lambda. See what insignts you can glean!</span>
<span class="comment">%</span>
<span class="comment">% Besides the usual array of model selection techniques (cross-validation,</span>
<span class="comment">% etc.) can you see any way to determine why you might prefer one method</span>
<span class="comment">% over the other (i.e. interpretability)?</span>

sigma = 0.01;

<span class="comment">%weights = randn(10, 1);</span>
weights = [1 0 1 0 1 0 0 0 0 0]';
y = X*weights + sigma*randn(size(x));

lambda = 0.001;

[B_lasso, finfo] = lasso(X, y, <span class="string">'lambda'</span>, lambda);
B_ridge = ridge(y, X, lambda);

figure; hold <span class="string">on</span>;
subplot <span class="string">141</span>; hold <span class="string">on</span>;
plot(x, y);
plot(x, X*B_lasso);
plot(x, X*B_ridge);
legend(<span class="string">'Noisy Data'</span>, <span class="string">'Lasso Fit'</span>, <span class="string">'Ridge Fit'</span>);

subplot <span class="string">142</span>;
stem(weights);
title(<span class="string">'True Coefficients'</span>);

subplot <span class="string">143</span>;
stem(B_ridge);
title(<span class="string">'Ridge Regression Coefficients'</span>);

subplot <span class="string">144</span>;
stem(B_lasso);
title(<span class="string">'Lasso Coefficients'</span>);
</pre><img vspace="5" hspace="5" src="lab_10_solutions_05.png" alt=""> <img vspace="5" hspace="5" src="lab_10_solutions_06.png" alt=""> <img vspace="5" hspace="5" src="lab_10_solutions_07.png" alt=""> <img vspace="5" hspace="5" src="lab_10_solutions_08.png" alt=""> <h2 id="3">Clustering</h2><p>The provided function mykmean takes three inputs (data, number of clusters, and maximum number of iterations) and returns 4 sets of values (center_ids (which maps each point to a cluster number), centers ( the converged centroid points), the number of iterations until convergence, and the total distance from points to their assigned centers).</p><p>First run the algorithm and visualize the results using the parameters and data defined below. Then play around with the data generation and algorithm. In particular qualitatively comment on how the clustering algorithm handles: clusters with different covariance structures (either shared or unique to each cluster) and a mismatch between the "true," number of clusters and the parameter k, and the total number of datapoints in each cluster.</p><pre class="codeinput"><span class="comment">% example parameters</span>
sigma_1 = [1, -0.5;-0.5, 2];
sigma_2 = [1, 0.5; 0.5, 1];
mu_1 = [2, 0]';
mu_2 = [0, -2]';

k = 2;
maxiter=200;
<span class="comment">% build data as sets of clusters, i.e.</span>
data_1 = ndRandn(mu_1, sigma_1, 1000);
data_2 = ndRandn(mu_2, sigma_2, 1000);
data = [data_1; data_2];
[ cids,centers,niter,alldist ] = mykmean(data,2, maxiter);

figure; hold <span class="string">on</span>;
<span class="keyword">for</span> kc = 1:2
    <span class="comment">% scatter points by cluster</span>
    scatter(data(cids==kc,1), data(cids==kc,2),<span class="string">'o'</span>)
    hold <span class="string">on</span>
<span class="keyword">end</span>

<span class="comment">% display cluster centers</span>
scatter(centers(:,1),centers(:,2),<span class="string">'s'</span>,<span class="string">'filled'</span>)
</pre><h2 id="4">Functions</h2><pre class="codeinput"><span class="keyword">function</span> samples = ndRandn(mean, cov, num)

    <span class="comment">% num parameter is optional</span>
    <span class="keyword">if</span> nargin &lt; 3
        num = 1;
    <span class="keyword">end</span>
    <span class="comment">% Standard Normal Data</span>
    D = randn(size(mean, 1), num);

    <span class="comment">% Take SVD of Cov</span>
    [U, S, V] = svd(cov); L = S.^(0.5);

    <span class="comment">% rotate standard normal samples by V, scale by sqrt of singular values</span>
    samples = V*L*D;

    <span class="comment">% shift by mean</span>
    samples = samples + mean;

    <span class="comment">% return transpose to fit specifications</span>
    samples = samples';


<span class="keyword">end</span>

<span class="keyword">function</span> none = boundaryViz(X1, X2)

none = 0;

<span class="comment">% determining the range for the search</span>
min_x = min(min(X1(:, 1)), min(X2(:, 1)));
min_y = min(min(X1(:, 2)), min(X2(:, 2)));
max_x = max(max(X1(:, 1)), max(X2(:, 1)));
max_y = max(max(X1(:, 2)), max(X2(:, 2)));

<span class="comment">% setting up the grid</span>
n_pts = 100;
margin_x = 0.05 * abs(max_x - min_x);
margin_y = 0.05 * abs(max_y - min_y);
x = linspace(min_x - margin_x, max_x + margin_x, n_pts);
y = linspace(min_y - margin_y, max_y + margin_y, n_pts);

[X, Y] = meshgrid(x, y);
Z = zeros(size(X));

<span class="comment">% Estimating the parameters for Gaussian model of data (Likelihood function)</span>
sig1 = cov(X1); sig2 = cov(X2);
mu1  = mean(X1); mu2 = mean(X2);

<span class="keyword">for</span> i = 1:n_pts
    <span class="keyword">for</span> j = 1:n_pts
        x0 = [x(i), y(j)]';

        <span class="comment">% This is the ratio of the log of the ratio of likelihoods</span>
        <span class="comment">% (convince yourself)</span>
        a = 0.5*(log(det(sig2)) - log(det(sig1)));
        b = -0.5 * (x0-mu1')'*pinv(sig1)*(x0-mu1');
        c = -0.5 * (x0-mu2')'*pinv(sig2)*(x0-mu2');

        Z(i, j) = a + b - c;
    <span class="keyword">end</span>

<span class="keyword">end</span>
figure; hold <span class="string">on</span>
<span class="comment">% the decision boundary is where log likelihoods are equal to each other</span>
contour(X, Y, Z', [0, 0]);
scatter(X1(:, 1), X1(:, 2));
scatter(X2(:, 1), X2(:, 2));
xlabel(<span class="string">'Neuron 1'</span>);
ylabel(<span class="string">'Neuron 2'</span>);
legend(<span class="string">'Decision Boundary'</span>, <span class="string">'Class 1'</span>, <span class="string">'Class 2'</span>);
axis <span class="string">equal</span>;


<span class="keyword">end</span>

<span class="keyword">function</span> [ cids,centers,niter,alldist ] = mykmean(data,ncluster,maxiter)
    <span class="comment">% AUTHOR: Ionotan Kuperwajs</span>
    <span class="keyword">if</span> nargin&lt;3
        maxiter=100 ;
    <span class="keyword">end</span>
    <span class="comment">% initialize</span>
    centers = data(randi(length(data),1,ncluster),:);
    cids = ones(length(data),1)*nan;
    alldist = [];
    <span class="keyword">for</span> niter = 1:maxiter
        <span class="comment">% recording the total distance</span>
        iterdist = 0;
        <span class="comment">% assign cluster for data</span>
        newcids = ones(length(data),1)*nan;
        <span class="keyword">for</span> k = 1:length(data) <span class="comment">% batch update: change everyone</span>
            d = data(k,:);
            [mindist,newcids(k)] = min(sqrt(sum((repmat(d,ncluster,1)-centers).^2,2)));
            iterdist= iterdist+sqrt(sum((d-centers(newcids(k),:)).^2));
        <span class="keyword">end</span>
        <span class="comment">% check if cid is stable now to terminate the search</span>
        <span class="keyword">if</span> sum(newcids~=cids)==0 <span class="comment">% check if the the cluster assignment is stable</span>
            alldist=[alldist,iterdist];
            <span class="keyword">break</span>
        <span class="keyword">end</span>
        cids = newcids;
        <span class="comment">% update center position</span>
        <span class="keyword">for</span> kc = 1:ncluster
            centers(kc,:) = mean(data(cids==kc,:),1);
        <span class="keyword">end</span>
        <span class="comment">% record dist</span>
        alldist=[alldist,iterdist];

    <span class="keyword">end</span>

<span class="keyword">end</span>
</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2021b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Lab 10: Classification, Regularization and Clustering
% 
% SOLUTIONS
%% Classification

% The function boundaryViz provided at the bottom of this script takes in
% two sets of data and follows the procedure outlined below in order to
% visualize a decision boundary between the two classes:
% 
% 1) Compute estimates of each groups covariance structure (the 
%    empirically observed covariance).
% 
% 2) For each point in a reasonable range, evaluate the likelihood a sample
%    said point came from either group, which is modeled as gaussian with 
%    mu = mu_obs, sigma = sigma_obs
%
% 3) Use the matlab contour function to draw the decision boundary
%
% Make sure you understand (roughly) the implementation of this function
% and how the example below works

sigma_1 = [1, -0.5;-0.5, 1];
sigma_2 = [1, 0.5; 0.5, 1];
mu_1 = [2, 1]';
mu_2 = [2, -2]';

X1 = ndRandn(mu_1, sigma_1, 500);
X2 = ndRandn(mu_2, sigma_2, 500);

 boundaryViz(X1, X2);

 % 1) Quadratic Discriminant Analysis
 % QDA assumes that the processing underlying the data generation for each
 % class is its own independent Gaussian distribution, but nothing else.
 % That is QDA solves the most general version of the classification
 % problem if we assume that each class is well modeled by a gaussian (with
 % each class having its own mean and covariance matrix)
 %
 % Use the functions ndRandn (should be familiar) and boundaryViz to
 % visualize some boundary curves in the general case (where the 2 classes 
 % have data drawn from completely unrelated normal distributions). 
 %
 % Start off with the settings below and then toggle the means,
 % covariances, and number of points used. Try to qualitatively explain
 % what you observe.
 
 % try these, then generate some of your own examples
sigma_1 = [1, -0.5;-0.5, 1];
sigma_2 = [1, 0.5; 0.5, 1];
sigma_3 = [0.3, 0.25; 0.25, 1];
mu_1 = [2, 1]';
mu_2 = [2, -2]';
mu_3 = [0, 0]'; 

n_pts = 500; % vary this
X1 = ndRandn(mu_1, sigma_1, n_pts);
X2 = ndRandn(mu_2, sigma_2, n_pts);
X3 = ndRandn(mu_3, sigma_3, n_pts);

% ANSWER: call this many times
boundaryViz(X1, X3);
 
 % 2) Fisher Linear Discriminant
 % The general problem above can be constrained by assuming that
 % the covariance strucuture is shared between the two classes (they have 
 % the same covariance matrix but different means). Now generate two sets
 % of data that fit this description and visualize the decision boundary.
 % As above first use the set of parameters provided below and then play
 % around with each (the shared covariance, mean values, and number of data
 % points) to get a sense for how each affects the solution. 
 
% now data from all classes should have same covariance 
n_pts = 500; % vary this
sigma = sigma_1;
X1 = ndRandn(mu_1, sigma, n_pts);
X2 = ndRandn(mu_2, sigma, n_pts);
X3 = ndRandn(mu_3, sigma, n_pts);
 
% ANSWER: call this many times
boundaryViz(X1, X3);

 % In class it was shown that this constraint is actually sufficient to
 % arrive at a closed for solution and that the optimal decision boundary 
 % is a linear surface (a line in 2-D). How well does this match the
 % visualizations you generated empirically above? Why? 
 
 % Answer: The decision boundary appears to be close to a plane most of the
 % time, but there is sometimes some noticable curve. This is because the
 % empirically inferred covariances from each class do not exactly meet the
 % assumptions for which the FLD is optimal. Indeed one of the central
 % difficulties of using QDA in high dimensions is that a large amount of
 % data is needed to accurately estimate the covariance matrix. On the
 % other hand, this is one of the clear benefits to using the special case
 % solutions: if the required assumptions are (approximately) true, you can
 % get a closer to optimal solution using less data than when applying the
 % unconstrained method. 
 
 
 % 3) The Prototype Classifier
 % Finally we can restrict the problem to the special case where the
 % classes share a covariance matrix and that covariance matrix is a scalar
 % multiple of the identity matrix. Generate data that according to this
 % assumption and visualize the resulting decision boundaries. (First use 
 % the provided parameters then...you know)
n_pts = 500; % vary this
s = 1.0;
sigma = [1.0, 0; 0, 1.0];

X1 = ndRandn(mu_1, sigma, n_pts);
X2 = ndRandn(mu_2, sigma, n_pts);
X3 = ndRandn(mu_3, sigma, n_pts);
 
boundaryViz(X2, X3);

 
 %  In class we saw that the MLE solution in this setting leads to a
 %  boundary surface that is the perpendicular bisector of the line segment
 %  between the class means. Does the method for computing boundary curves
 %  employed here seem to reliably find that solution? 
 
 % Answer: Similar to above. The quality of the QDA as it pertains to
 % approaching the prototype classifier on this data (which we know to be 
 % optimal) depends on the ability to accurately estimate the covariance
 % matrices from the data. 
%% Regularization

% Below is an example meant to demonstrate the syntax used by matlab's
% built in functions for ridge and lasso regression
x = -1:0.01:1.0; 
x = x';
X = [x, x.^2, x.^3, x.^4, x.^5, x.^6, x.^7, x.^8, x.^9, x.^10]; 
weights = randn(11, 1);
weights = [1 0 1 0 1 0 0 0 0 0]';
y = X*weights + 0.5*randn(size(x));

[B_lasso, finfo] = lasso(X, y, 'lambda', 0.03, 'Intercept', true);
B_lasso;
lambda = 0.01;
B_ridge = ridge(y, X, lambda);
B_ridge;

figure; hold on; 
plot(x, y);
plot(x, X*B_lasso);
plot(x, X*B_ridge);
legend('Noisy Data', 'Lasso Fit', 'Ridge Fit');

% 1) Ridge Regression
% We've shown previously optimizing mean square error is equivalent to
% solving MLE if one assumes a gaussian model for the data (with the key 
% observation being that taking a log of a gaussian likelihood function 
% leaves you with a squared error term). More recently we saw that adding
% an additive L2 penalty to the objective function is equivalent to
% imposing a Gaussian prior on the parameters and then performing MAP
% estimation. 
%
% First we will tease apart what it means to "enforce some prior," by
% putting the model in a situation where it only has the information
% provided by its prior to decide on a set of parameters: fitting noise. 
% Fill in the skeleton code below and explain the results.

% define regressor features (note lack of y-intercept)
x = [-1:0.01:1]';
X = [x, x.^2, x.^3, x.^4, x.^5, x.^6, x.^7, x.^8, x.^9, x.^10];

% define lambda
lambda = 0.1; 

n_expts = 1000; 
running_list_of_all_coeffs = [];
for ii = 1:n_expts
    % make some noise to fit
    y = randn(size(x));
    
    % For the purpose of this lab, you are free to use either the closed \
    % form solutionfound in the slides from class or matlab's built-in
    % function. On the HW you should compute the solution for ridge
    % regression yourself. 
    B = 0; % REPLACE THIS LINE
    
    %Answer
    B = ridge(y, X, lambda);
    running_list_of_all_coeffs = [running_list_of_all_coeffs, B'];
    
end

figure;
hist(running_list_of_all_coeffs, 200);

% Does the shape of the distriubition of parameters make sense given the
% MAP interpretation of ridge regression?

% 2) LASSO
% We could instead choose to penalize using the L1 norm of the parameters,
% doing so is the basis of LASSO regression. What difference does this
% choice make? Consider the unit circle: all points on the circle by
% definition all have the same L2 norm, but the four axis intercepts have
% minimal L1 norms. Thus we might expect LASSO to prefer solutions where
% some parameters are pushed towars exactly zero. 

% Repeat the noise fitting experiment above, but this time use Lasso and
% lambda = 0.001

% ANSWER: 
n_expts = 1000; 
running_list_of_all_coeffs = [];
for ii = 1:n_expts
    % make some noise to fit
    y = randn(size(x));
    
    % For the purpose of this lab, you are free to use either the closed \
    % form solutionfound in the slides from class or matlab's built-in
    % function. On the HW you should compute the solution for ridge
    % regression yourself. 
    B = 0; % REPLACE THIS LINE
    
    %Answer
    B = lasso(X, y, 'lambda', 0.001);
    running_list_of_all_coeffs = [running_list_of_all_coeffs, B'];
    
end
figure;
hist(running_list_of_all_coeffs, 200);

% Does the shape of the distriubition of parameters make sense given the
% MAP interpretation of LASSO?

% 3) Behavior with variable lambda and SNR
% Change between different weight settings, values for the variable sigma, 
% and values of lambda. See what insignts you can glean!
%
% Besides the usual array of model selection techniques (cross-validation, 
% etc.) can you see any way to determine why you might prefer one method
% over the other (i.e. interpretability)? 

sigma = 0.01;

%weights = randn(10, 1);
weights = [1 0 1 0 1 0 0 0 0 0]';
y = X*weights + sigma*randn(size(x));

lambda = 0.001;

[B_lasso, finfo] = lasso(X, y, 'lambda', lambda);
B_ridge = ridge(y, X, lambda);

figure; hold on; 
subplot 141; hold on;
plot(x, y);
plot(x, X*B_lasso);
plot(x, X*B_ridge);
legend('Noisy Data', 'Lasso Fit', 'Ridge Fit');

subplot 142; 
stem(weights);
title('True Coefficients');

subplot 143; 
stem(B_ridge);
title('Ridge Regression Coefficients');

subplot 144; 
stem(B_lasso);
title('Lasso Coefficients');


%% Clustering
% The provided function mykmean takes three inputs (data, number of 
% clusters, and maximum number of iterations) and returns 4 sets of values
% (center_ids (which maps each point to a cluster number), centers (
% the converged centroid points), the number of iterations until 
% convergence, and the total distance from points to their assigned 
% centers). 
%
% First run the algorithm and visualize the results using the parameters
% and data defined below. Then play around with the data generation and
% algorithm. In particular qualitatively comment on how the clustering
% algorithm handles: clusters with different covariance structures (either
% shared or unique to each cluster) and a mismatch between the "true,"
% number of clusters and the parameter k, and the total number of
% datapoints in each cluster. 

% example parameters
sigma_1 = [1, -0.5;-0.5, 2];
sigma_2 = [1, 0.5; 0.5, 1];
mu_1 = [2, 0]';
mu_2 = [0, -2]';

k = 2;
maxiter=200;
% build data as sets of clusters, i.e. 
data_1 = ndRandn(mu_1, sigma_1, 1000);
data_2 = ndRandn(mu_2, sigma_2, 1000);
data = [data_1; data_2];
[ cids,centers,niter,alldist ] = mykmean(data,2, maxiter);

figure; hold on;
for kc = 1:2
    % scatter points by cluster
    scatter(data(cids==kc,1), data(cids==kc,2),'o')
    hold on
end

% display cluster centers
scatter(centers(:,1),centers(:,2),'s','filled')

%% Functions
function samples = ndRandn(mean, cov, num)

    % num parameter is optional
    if nargin < 3
        num = 1; 
    end
    % Standard Normal Data
    D = randn(size(mean, 1), num);
    
    % Take SVD of Cov
    [U, S, V] = svd(cov); L = S.^(0.5);
    
    % rotate standard normal samples by V, scale by sqrt of singular values
    samples = V*L*D;
    
    % shift by mean
    samples = samples + mean;
    
    % return transpose to fit specifications
    samples = samples';
    
  
end

function none = boundaryViz(X1, X2)

none = 0;

% determining the range for the search
min_x = min(min(X1(:, 1)), min(X2(:, 1)));
min_y = min(min(X1(:, 2)), min(X2(:, 2)));
max_x = max(max(X1(:, 1)), max(X2(:, 1)));
max_y = max(max(X1(:, 2)), max(X2(:, 2)));

% setting up the grid 
n_pts = 100;
margin_x = 0.05 * abs(max_x - min_x);
margin_y = 0.05 * abs(max_y - min_y);
x = linspace(min_x - margin_x, max_x + margin_x, n_pts); 
y = linspace(min_y - margin_y, max_y + margin_y, n_pts);

[X, Y] = meshgrid(x, y);
Z = zeros(size(X));

% Estimating the parameters for Gaussian model of data (Likelihood function)
sig1 = cov(X1); sig2 = cov(X2); 
mu1  = mean(X1); mu2 = mean(X2); 

for i = 1:n_pts
    for j = 1:n_pts
        x0 = [x(i), y(j)]';
        
        % This is the ratio of the log of the ratio of likelihoods
        % (convince yourself)
        a = 0.5*(log(det(sig2)) - log(det(sig1)));
        b = -0.5 * (x0-mu1')'*pinv(sig1)*(x0-mu1');
        c = -0.5 * (x0-mu2')'*pinv(sig2)*(x0-mu2');
        
        Z(i, j) = a + b - c; 
    end

end
figure; hold on
% the decision boundary is where log likelihoods are equal to each other
contour(X, Y, Z', [0, 0]);
scatter(X1(:, 1), X1(:, 2)); 
scatter(X2(:, 1), X2(:, 2)); 
xlabel('Neuron 1');
ylabel('Neuron 2');
legend('Decision Boundary', 'Class 1', 'Class 2');
axis equal; 


end

function [ cids,centers,niter,alldist ] = mykmean(data,ncluster,maxiter)
    % AUTHOR: Ionotan Kuperwajs
    if nargin<3
        maxiter=100 ;
    end
    % initialize
    centers = data(randi(length(data),1,ncluster),:);
    cids = ones(length(data),1)*nan;
    alldist = [];
    for niter = 1:maxiter
        % recording the total distance
        iterdist = 0;
        % assign cluster for data
        newcids = ones(length(data),1)*nan;
        for k = 1:length(data) % batch update: change everyone
            d = data(k,:);
            [mindist,newcids(k)] = min(sqrt(sum((repmat(d,ncluster,1)-centers).^2,2)));
            iterdist= iterdist+sqrt(sum((d-centers(newcids(k),:)).^2));
        end
        % check if cid is stable now to terminate the search
        if sum(newcids~=cids)==0 % check if the the cluster assignment is stable
            alldist=[alldist,iterdist];
            break
        end
        cids = newcids;
        % update center position
        for kc = 1:ncluster
            centers(kc,:) = mean(data(cids==kc,:),1);
        end
        % record dist
        alldist=[alldist,iterdist];
        
    end

end


##### SOURCE END #####
--></body></html>