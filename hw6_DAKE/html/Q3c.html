
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Q3c</title><meta name="generator" content="MATLAB 9.11"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2021-12-20"><meta name="DC.source" content="Q3c.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#4">prototype classifier</a></li><li><a href="#6">Fischer Linear Discriminant</a></li><li><a href="#8">Quadratic Classifier</a></li><li><a href="#14">Functions</a></li></ul></div><pre class="codeinput">clear; close <span class="string">all</span>; clc;
</pre><p>Loading the datasets and segregating them based on conditions:</p><pre class="codeinput">load(<span class="string">'experimentData.mat'</span>)
load(<span class="string">'newMeasurements.mat'</span>)

cond1_index = find(trialConds == 1);
cond2_index = find(trialConds == 2);
data1 = data(cond1_index, :);
data2 = data(cond2_index, :);

test1_index = find(newConds == 1);
test2_index = find(newConds == 2);
test1 = newMeasurements(test1_index, :);
test2 = newMeasurements(test2_index, :);
</pre><h2 id="4">prototype classifier</h2><pre class="codeinput">mean_data1 = mean(data1); <span class="comment">% mean of old data condition 1</span>
mean_data2 = mean(data2); <span class="comment">% mean of old data condition 2</span>
w = mean_data2 - mean_data1; <span class="comment">% Discriminant vector</span>
w_norm = sqrt(sum(w.^2)); <span class="comment">% Norm of discriminant vector</span>
w_hat = w./w_norm; <span class="comment">% Normalized discriminant vector</span>
midpoint_data = (mean_data1 + mean_data2)/2; <span class="comment">% Midpoint of means of datasets</span>

figure();
scatter3(data1(:, 1), data1(:, 2), data1(:, 3), 10, <span class="string">'r'</span>, <span class="keyword">...</span>
    <span class="string">'filled'</span>, <span class="string">'DisplayName'</span>, <span class="string">'control odorant'</span>)
hold <span class="string">on</span>;
scatter3(data2(:, 1), data2(:, 2), data2(:, 3), 10, <span class="string">'k'</span>, <span class="keyword">...</span>
    <span class="string">'filled'</span>, <span class="string">'DisplayName'</span>, <span class="string">'pumpkin-spice odorant'</span>)
legend();
xlabel(<span class="string">'Voxel 1'</span>); ylabel(<span class="string">'Voxel 2'</span>); zlabel(<span class="string">'Voxel 3'</span>);
title(<span class="string">'Responses in three voxels across 2 trial conditions'</span>)

plot3([midpoint_data(1) - w_hat(1)/2, midpoint_data(1) + w_hat(1)/2], <span class="keyword">...</span>
    [midpoint_data(2) - w_hat(2)/2, midpoint_data(2) + w_hat(2)/2], <span class="keyword">...</span>
    [midpoint_data(3) - w_hat(3)/2, midpoint_data(3) + w_hat(3)/2], <span class="keyword">...</span>
    <span class="string">'m-'</span>, <span class="string">'DisplayName'</span>, <span class="string">'w hat'</span>, <span class="string">'LineWidth'</span>, 1.5)
axis <span class="string">equal</span>;

x_dec_boundary = xlim; <span class="comment">% x limits of the graph</span>
<span class="comment">% Computing decision boundary given w_hat</span>
y_dec_boundary = ylim;
z_dec_boundary = decision_boundary(x_dec_boundary, y_dec_boundary, <span class="keyword">...</span>
    midpoint_data, w_hat);

<span class="comment">% Computing decision boundary plane given the discriminant vector and the</span>
<span class="comment">% midpoint</span>
pointsss = 1e2;
x_range = linspace(x_dec_boundary(1), x_dec_boundary(2), pointsss);
y_range = linspace(y_dec_boundary(1), y_dec_boundary(2), pointsss);
z_range = zeros(pointsss, 1);
<span class="keyword">for</span> zz = 1:pointsss
    z_range(zz) = decision_boundary(x_range(zz), y_range(zz), <span class="keyword">...</span>
        midpoint_data, w_hat);
<span class="keyword">end</span>

<span class="comment">% Creating a matrix of datapoints that lie on the decision boundary</span>
[x, y] = meshgrid(x_range, y_range);
z = zeros(pointsss, pointsss);
<span class="keyword">for</span> zz = 1:pointsss
    <span class="keyword">for</span> ff = 1:pointsss
        z(zz, ff) = decision_boundary(x(zz, ff), y(zz, ff), <span class="keyword">...</span>
            midpoint_data, w_hat);
    <span class="keyword">end</span>
<span class="keyword">end</span>
surf(x, y, z, <span class="string">'FaceColor'</span>, <span class="string">'#EDB120'</span>, <span class="string">'EdgeColor'</span>,<span class="string">'#EDB120'</span>, <span class="string">'FaceAlpha'</span>, <span class="keyword">...</span>
    0.5, <span class="string">'EdgeAlpha'</span>, 0.5, <span class="string">'DisplayName'</span>, <span class="string">'Decision boundary'</span>);
set(gca, <span class="string">'FontSize'</span>, 14)
set(gca, <span class="string">'LineWidth'</span>, 2)
title(<span class="string">'Prototype Classifier'</span>)
legend(<span class="string">'Location'</span>, <span class="string">'northeastoutside'</span>);

<span class="comment">% Computing fraction correctly classified by the classifier on the train</span>
<span class="comment">% and test sets</span>
frac_correctly_classified_train = classification_performance(data1, <span class="keyword">...</span>
    data2, x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat)

frac_correctly_classified_test = classification_performance(test1, <span class="keyword">...</span>
    test2, x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat)
</pre><img vspace="5" hspace="5" src="Q3c_01.png" alt=""> <p>We know from HW4 that the two datasets have different covariance matrices and hence the prototype classifer is not a great choice. We also known from class and Q2 of this assignment, that a Quadratic Discriminant Classifier (QDA) is better suited for such datasets. For sanity check, we can run the regularized Fischer and then finally compare it with the QDA.</p><h2 id="6">Fischer Linear Discriminant</h2><pre class="codeinput">lambdas = 0:0.05:1;
frac_correctly_classified_train_cv = zeros(length(lambdas), 1);
frac_correctly_classified_test_cv = zeros(length(lambdas), 1);

<span class="keyword">for</span> ll = 1:length(lambdas)
    lambda = lambdas(ll);

    <span class="comment">% computing discriminant vector for regularized Fischer</span>
    cov_data1 = cov(data1); cov_data2 = cov(data2);
    cov_combined = (cov_data1 + cov_data2)/2;
    cov_estimated = (1 - lambda) .* cov_combined + lambda .* eye(3);
    w_hat_estim = cov_estimated \ w';

    <span class="comment">% Decision boundary</span>
    z_dec_boundary = decision_boundary(x_dec_boundary, y_dec_boundary, <span class="keyword">...</span>
        midpoint_data, w_hat_estim);

    <span class="comment">% Computing fraction correctly classified by the classifier on the train</span>
    <span class="comment">% and test sets</span>
    frac_correctly_classified_train_cv(ll) = classification_performance(data1, <span class="keyword">...</span>
        data2, x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat_estim);
    frac_correctly_classified_test_cv(ll) = classification_performance(test1, <span class="keyword">...</span>
        test2, x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat_estim);
<span class="keyword">end</span>

figure()
plot(lambdas, frac_correctly_classified_train_cv, <span class="string">'ro-'</span>, <span class="string">'DisplayName'</span>, <span class="string">'train'</span>)
hold <span class="string">on</span>;
plot(lambdas, frac_correctly_classified_test_cv, <span class="string">'b*-'</span>, <span class="string">'DisplayName'</span>, <span class="string">'test'</span>)
xlabel(<span class="string">'\lambda'</span>)
ylabel(<span class="string">'proportion correctly classified'</span>)
title(<span class="string">"Fischer's Linear Discriminant Classifier cross-validation"</span>)
legend()
</pre><img vspace="5" hspace="5" src="Q3c_02.png" alt=""> <p>We can see that train accuracy is always higher than testing accuracy and both fall as a function of lambda. lambda = 0 seems to perform the best and hence Fischer's Linear Discriminant appears to be doing a great job on this dataset.</p><h2 id="8">Quadratic Classifier</h2><p>Running a quadratic classifier:</p><pre class="codeinput">figure()
scatter3(data1(:, 1), data1(:, 2), data1(:, 3), 10, <span class="string">'r'</span>, <span class="keyword">...</span>
    <span class="string">'filled'</span>, <span class="string">'DisplayName'</span>, <span class="string">'control odorant'</span>)
hold <span class="string">on</span>;
scatter3(data2(:, 1), data2(:, 2), data2(:, 3), 10, <span class="string">'k'</span>, <span class="keyword">...</span>
    <span class="string">'filled'</span>, <span class="string">'DisplayName'</span>, <span class="string">'pumpkin-spice odorant'</span>)
legend();
xlabel(<span class="string">'Voxel 1'</span>); ylabel(<span class="string">'Voxel 2'</span>); zlabel(<span class="string">'Voxel 3'</span>);
title(<span class="string">'Responses in three voxels across 2 trial conditions'</span>)

<span class="comment">% range of datapoints that span the space</span>
xx_ = xlim;
yy_ = ylim;
zz_ = zlim;
pps = 40;
xx = linspace(xx_(1), xx_(2), pps);
yy = linspace(yy_(1), yy_(2), pps);
zz = linspace(zz_(1), zz_(2), pps);
[X, Y, Z] = meshgrid(xx, yy, zz);
XYZ = [X(:) Y(:) Z(:)];

<span class="comment">% computing probability for each of these datapoints belonging to either</span>
<span class="comment">% cluster</span>
p1 = mvnpdf(XYZ, mean_data1, cov_data1);
p2 = mvnpdf(XYZ, mean_data2, cov_data2);

diff_p = p1 - p2;
aa = abs(diff_p);
thresh = 0.01 * max(aa);
scatter3(XYZ(aa&gt;thresh, 1), XYZ(aa&gt;thresh, 2), XYZ(aa&gt;thresh, 3), [], <span class="keyword">...</span>
    aa(aa&gt;thresh), <span class="string">'MarkerFaceAlpha'</span>, 1, <span class="keyword">...</span>
    <span class="string">'MarkerEdgeAlpha'</span>, 0.4)
</pre><img vspace="5" hspace="5" src="Q3c_03.png" alt=""> <p>Next we compute the probability of each datapoint coming from a Gaussian distribution given by the mean and covariance of either cluster. The quadratic classifier will call the cluster based on whether the probability of one is greater than the other.</p><pre class="codeinput">p1_train1 = mvnpdf(data1, mean_data1, cov_data1);
p2_train1 = mvnpdf(data1, mean_data2, cov_data2);
p1_train2 = mvnpdf(data2, mean_data1, cov_data1);
p2_train2 = mvnpdf(data2, mean_data2, cov_data2);

correct_train1 = sum(p1_train1 &gt; p2_train1);
correct_train2 = sum(p2_train2 &gt; p1_train2);

frac_correctly_classified_train = (correct_train1 + correct_train2)./<span class="keyword">...</span>
    (size(data1, 1) + size(data2, 1))
</pre><pre class="codeoutput">
frac_correctly_classified_train =

    0.9314

</pre><p>Repeating the same for the test set.</p><pre class="codeinput">p1_test1 = mvnpdf(test1, mean_data1, cov_data1);
p2_test1 = mvnpdf(test1, mean_data2, cov_data2);
p1_test2 = mvnpdf(test2, mean_data1, cov_data1);
p2_test2 = mvnpdf(test2, mean_data2, cov_data2);

correct_test1 = sum(p1_test1 &gt; p2_test1);
correct_test2 = sum(p2_test2 &gt; p1_test2);

frac_correctly_classified_test = (correct_test1 + correct_test2)./<span class="keyword">...</span>
    (size(test1, 1) + size(test2, 1))
</pre><pre class="codeoutput">
frac_correctly_classified_test =

    0.9300

</pre><p>Overall we can see that QDA performs the best on both the train and the test sets and therefore, does not end up overfitting the training data as happened in the case of Fischer Linear Discriminant. It is obvious that this classifier would fit the best as it has more free parameters. However, the choice of the classifier is justified because the two datasets have different covariances. Prototype and Fischer assume the covariances of the two clusters to be the same and hence are not a great choice for this dataset. We can also see that the difference between the training and the testing error is not that great for QDA as is the case for the other linear classifiers tested.</p><h2 id="14">Functions</h2><pre class="codeinput"><span class="keyword">function</span> z_dec_boundary = decision_boundary(x_dec_boundary, y_dec_boundary, <span class="keyword">...</span>
    midpoint_data, w_hat)
    <span class="comment">% The function computes a decision boundary given the discriminant</span>
    <span class="comment">% vector. It uses the information that decision boundary plane is</span>
    <span class="comment">% perpendicular to the discriminant vector and that it passes through</span>
    <span class="comment">% the midpoint between the means of two datasets. The equation of a</span>
    <span class="comment">% plane can then be used to compute the decision boundary.</span>

    z_dec_boundary = midpoint_data(3) - (w_hat(2)/w_hat(3)) * (y_dec_boundary - <span class="keyword">...</span>
        midpoint_data(2)) - (w_hat(1)/w_hat(3)) * (x_dec_boundary - <span class="keyword">...</span>
        midpoint_data(1));
<span class="keyword">end</span>

<span class="keyword">function</span> frac_correctly_classified = classification_performance(data1, data2, <span class="keyword">...</span>
    x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat)
    <span class="comment">% The function computes the fraction of datapoints that are correctly</span>
    <span class="comment">% classified by the given linear classifier. It computes the expected</span>
    <span class="comment">% z-coord given x and y coords of each datapoint were that datapoint to lie on</span>
    <span class="comment">% the decision boundary. It then uses the predicted z-value and the</span>
    <span class="comment">% actual z-value to decide the class of the datapoint and checks it with</span>
    <span class="comment">% the true class of the datapoint.</span>

    x1_ones = ones(size(data1(:, 1))) .* x_dec_boundary(1);
    y1_ones = ones(size(data1(:, 1))) .* y_dec_boundary(1);
    z1_ones = ones(size(data1(:, 1))) .* z_dec_boundary(1);
    x2_ones = ones(size(data2(:, 1))) .* x_dec_boundary(1);
    y2_ones = ones(size(data2(:, 1))) .* y_dec_boundary(1);
    z2_ones = ones(size(data2(:, 1))) .* z_dec_boundary(1);

    z_data1 = z1_ones - (w_hat(2)/w_hat(3)) * (data1(:, 2) - y1_ones) - <span class="keyword">...</span>
        (w_hat(1)/w_hat(3)) * (data1(:, 1) - x1_ones);
    z_data2 = z2_ones - (w_hat(2)/w_hat(3)) * (data2(:, 2) - y2_ones) - <span class="keyword">...</span>
        (w_hat(1)/w_hat(3)) * (data2(:, 1) - x2_ones);

    correct_data1 = sum(z_data1 &gt;= data1(:, 3));
    correct_data2 = sum(z_data2 &lt;= data2(:, 3));

    frac_correctly_classified = (correct_data1 + correct_data2)./<span class="keyword">...</span>
        (size(data1, 1) + size(data2, 1));
<span class="keyword">end</span>
</pre><pre class="codeoutput">
frac_correctly_classified_train =

    0.8807


frac_correctly_classified_test =

    0.8300

</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2021b</a><br></p></div><!--
##### SOURCE BEGIN #####
clear; close all; clc;

%%
% Loading the datasets and segregating them based on conditions:
%%
load('experimentData.mat')
load('newMeasurements.mat')

cond1_index = find(trialConds == 1);
cond2_index = find(trialConds == 2);
data1 = data(cond1_index, :);
data2 = data(cond2_index, :);

test1_index = find(newConds == 1);
test2_index = find(newConds == 2);
test1 = newMeasurements(test1_index, :);
test2 = newMeasurements(test2_index, :);

%% prototype classifier

mean_data1 = mean(data1); % mean of old data condition 1
mean_data2 = mean(data2); % mean of old data condition 2
w = mean_data2 - mean_data1; % Discriminant vector
w_norm = sqrt(sum(w.^2)); % Norm of discriminant vector
w_hat = w./w_norm; % Normalized discriminant vector
midpoint_data = (mean_data1 + mean_data2)/2; % Midpoint of means of datasets

figure();
scatter3(data1(:, 1), data1(:, 2), data1(:, 3), 10, 'r', ...
    'filled', 'DisplayName', 'control odorant')
hold on;
scatter3(data2(:, 1), data2(:, 2), data2(:, 3), 10, 'k', ...
    'filled', 'DisplayName', 'pumpkin-spice odorant')
legend();
xlabel('Voxel 1'); ylabel('Voxel 2'); zlabel('Voxel 3');
title('Responses in three voxels across 2 trial conditions')

plot3([midpoint_data(1) - w_hat(1)/2, midpoint_data(1) + w_hat(1)/2], ...
    [midpoint_data(2) - w_hat(2)/2, midpoint_data(2) + w_hat(2)/2], ...
    [midpoint_data(3) - w_hat(3)/2, midpoint_data(3) + w_hat(3)/2], ...
    'm-', 'DisplayName', 'w hat', 'LineWidth', 1.5)
axis equal;

x_dec_boundary = xlim; % x limits of the graph
% Computing decision boundary given w_hat
y_dec_boundary = ylim;
z_dec_boundary = decision_boundary(x_dec_boundary, y_dec_boundary, ...
    midpoint_data, w_hat);

% Computing decision boundary plane given the discriminant vector and the
% midpoint
pointsss = 1e2;
x_range = linspace(x_dec_boundary(1), x_dec_boundary(2), pointsss);
y_range = linspace(y_dec_boundary(1), y_dec_boundary(2), pointsss);
z_range = zeros(pointsss, 1);
for zz = 1:pointsss
    z_range(zz) = decision_boundary(x_range(zz), y_range(zz), ...
        midpoint_data, w_hat);
end

% Creating a matrix of datapoints that lie on the decision boundary
[x, y] = meshgrid(x_range, y_range);
z = zeros(pointsss, pointsss);
for zz = 1:pointsss
    for ff = 1:pointsss
        z(zz, ff) = decision_boundary(x(zz, ff), y(zz, ff), ...
            midpoint_data, w_hat);
    end
end
surf(x, y, z, 'FaceColor', '#EDB120', 'EdgeColor','#EDB120', 'FaceAlpha', ...
    0.5, 'EdgeAlpha', 0.5, 'DisplayName', 'Decision boundary');
set(gca, 'FontSize', 14)
set(gca, 'LineWidth', 2)
title('Prototype Classifier')
legend('Location', 'northeastoutside');

% Computing fraction correctly classified by the classifier on the train
% and test sets
frac_correctly_classified_train = classification_performance(data1, ...
    data2, x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat)

frac_correctly_classified_test = classification_performance(test1, ...
    test2, x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat)

%%
% We know from HW4 that the two datasets have different covariance matrices
% and hence the prototype classifer is not a great choice. We also known
% from class and Q2 of this assignment, that a Quadratic Discriminant
% Classifier (QDA) is better suited for such datasets. For sanity check, we
% can run the regularized Fischer and then finally compare it with the QDA.

%% Fischer Linear Discriminant
lambdas = 0:0.05:1;
frac_correctly_classified_train_cv = zeros(length(lambdas), 1);
frac_correctly_classified_test_cv = zeros(length(lambdas), 1);

for ll = 1:length(lambdas)
    lambda = lambdas(ll);
    
    % computing discriminant vector for regularized Fischer
    cov_data1 = cov(data1); cov_data2 = cov(data2);
    cov_combined = (cov_data1 + cov_data2)/2;
    cov_estimated = (1 - lambda) .* cov_combined + lambda .* eye(3);
    w_hat_estim = cov_estimated \ w';

    % Decision boundary
    z_dec_boundary = decision_boundary(x_dec_boundary, y_dec_boundary, ...
        midpoint_data, w_hat_estim);
    
    % Computing fraction correctly classified by the classifier on the train
    % and test sets
    frac_correctly_classified_train_cv(ll) = classification_performance(data1, ...
        data2, x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat_estim);
    frac_correctly_classified_test_cv(ll) = classification_performance(test1, ...
        test2, x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat_estim);
end

figure()
plot(lambdas, frac_correctly_classified_train_cv, 'ro-', 'DisplayName', 'train')
hold on;
plot(lambdas, frac_correctly_classified_test_cv, 'b*-', 'DisplayName', 'test')
xlabel('\lambda')
ylabel('proportion correctly classified')
title("Fischer's Linear Discriminant Classifier cross-validation")
legend()

%%
% We can see that train accuracy is always higher than testing accuracy and
% both fall as a function of lambda. lambda = 0 seems to perform the best
% and hence Fischer's Linear Discriminant appears to be doing a great job
% on this dataset.

%% Quadratic Classifier
% Running a quadratic classifier:
%%
figure()
scatter3(data1(:, 1), data1(:, 2), data1(:, 3), 10, 'r', ...
    'filled', 'DisplayName', 'control odorant')
hold on;
scatter3(data2(:, 1), data2(:, 2), data2(:, 3), 10, 'k', ...
    'filled', 'DisplayName', 'pumpkin-spice odorant')
legend();
xlabel('Voxel 1'); ylabel('Voxel 2'); zlabel('Voxel 3');
title('Responses in three voxels across 2 trial conditions')

% range of datapoints that span the space
xx_ = xlim;
yy_ = ylim;
zz_ = zlim;
pps = 40;
xx = linspace(xx_(1), xx_(2), pps);
yy = linspace(yy_(1), yy_(2), pps);
zz = linspace(zz_(1), zz_(2), pps);
[X, Y, Z] = meshgrid(xx, yy, zz);
XYZ = [X(:) Y(:) Z(:)];

% computing probability for each of these datapoints belonging to either
% cluster
p1 = mvnpdf(XYZ, mean_data1, cov_data1);
p2 = mvnpdf(XYZ, mean_data2, cov_data2);

diff_p = p1 - p2;
aa = abs(diff_p);
thresh = 0.01 * max(aa);
scatter3(XYZ(aa>thresh, 1), XYZ(aa>thresh, 2), XYZ(aa>thresh, 3), [], ...
    aa(aa>thresh), 'MarkerFaceAlpha', 1, ...
    'MarkerEdgeAlpha', 0.4)

%%
% Next we compute the probability of each datapoint coming from a Gaussian
% distribution given by the mean and covariance of either cluster. The
% quadratic classifier will call the cluster based on whether the
% probability of one is greater than the other.
%%
p1_train1 = mvnpdf(data1, mean_data1, cov_data1);
p2_train1 = mvnpdf(data1, mean_data2, cov_data2);
p1_train2 = mvnpdf(data2, mean_data1, cov_data1);
p2_train2 = mvnpdf(data2, mean_data2, cov_data2);

correct_train1 = sum(p1_train1 > p2_train1);
correct_train2 = sum(p2_train2 > p1_train2);

frac_correctly_classified_train = (correct_train1 + correct_train2)./...
    (size(data1, 1) + size(data2, 1))
%%
% Repeating the same for the test set.
p1_test1 = mvnpdf(test1, mean_data1, cov_data1);
p2_test1 = mvnpdf(test1, mean_data2, cov_data2);
p1_test2 = mvnpdf(test2, mean_data1, cov_data1);
p2_test2 = mvnpdf(test2, mean_data2, cov_data2);

correct_test1 = sum(p1_test1 > p2_test1);
correct_test2 = sum(p2_test2 > p1_test2);

frac_correctly_classified_test = (correct_test1 + correct_test2)./...
    (size(test1, 1) + size(test2, 1))

%%
% Overall we can see that QDA performs the best on both the train and the
% test sets and therefore, does not end up overfitting the training data as
% happened in the case of Fischer Linear Discriminant. It is obvious that
% this classifier would fit the best as it has more free parameters.
% However, the choice of the classifier is justified because the two
% datasets have different covariances. Prototype and Fischer assume the
% covariances of the two clusters to be the same and hence are not a great
% choice for this dataset. We can also see that the difference between the
% training and the testing error is not that great for QDA as is the case
% for the other linear classifiers tested.

%% Functions
function z_dec_boundary = decision_boundary(x_dec_boundary, y_dec_boundary, ...
    midpoint_data, w_hat)
    % The function computes a decision boundary given the discriminant
    % vector. It uses the information that decision boundary plane is
    % perpendicular to the discriminant vector and that it passes through
    % the midpoint between the means of two datasets. The equation of a
    % plane can then be used to compute the decision boundary.

    z_dec_boundary = midpoint_data(3) - (w_hat(2)/w_hat(3)) * (y_dec_boundary - ...
        midpoint_data(2)) - (w_hat(1)/w_hat(3)) * (x_dec_boundary - ...
        midpoint_data(1));
end

function frac_correctly_classified = classification_performance(data1, data2, ...
    x_dec_boundary, y_dec_boundary, z_dec_boundary, w_hat)
    % The function computes the fraction of datapoints that are correctly
    % classified by the given linear classifier. It computes the expected
    % z-coord given x and y coords of each datapoint were that datapoint to lie on
    % the decision boundary. It then uses the predicted z-value and the
    % actual z-value to decide the class of the datapoint and checks it with
    % the true class of the datapoint.

    x1_ones = ones(size(data1(:, 1))) .* x_dec_boundary(1);
    y1_ones = ones(size(data1(:, 1))) .* y_dec_boundary(1);
    z1_ones = ones(size(data1(:, 1))) .* z_dec_boundary(1);
    x2_ones = ones(size(data2(:, 1))) .* x_dec_boundary(1);
    y2_ones = ones(size(data2(:, 1))) .* y_dec_boundary(1);
    z2_ones = ones(size(data2(:, 1))) .* z_dec_boundary(1);
    
    z_data1 = z1_ones - (w_hat(2)/w_hat(3)) * (data1(:, 2) - y1_ones) - ...
        (w_hat(1)/w_hat(3)) * (data1(:, 1) - x1_ones);
    z_data2 = z2_ones - (w_hat(2)/w_hat(3)) * (data2(:, 2) - y2_ones) - ...
        (w_hat(1)/w_hat(3)) * (data2(:, 1) - x2_ones);
    
    correct_data1 = sum(z_data1 >= data1(:, 3));
    correct_data2 = sum(z_data2 <= data2(:, 3));
    
    frac_correctly_classified = (correct_data1 + correct_data2)./...
        (size(data1, 1) + size(data2, 1));
end
##### SOURCE END #####
--></body></html>