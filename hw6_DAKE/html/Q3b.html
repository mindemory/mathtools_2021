
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Q3b</title><meta name="generator" content="MATLAB 9.11"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2021-12-18"><meta name="DC.source" content="Q3b.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#9">Functions</a></li></ul></div><pre class="codeinput">clear; close <span class="string">all</span>; clc;
</pre><p>The extracellular neural recordings suggested that they arose from either 3 or 4 neurons using the preliminary analyses done in HW2. With the new tools of clustering, we now have a more elegant way of running the analyses to determine the number of neurons that are responsible for the data. First, we project the higher dimensional data onto a lower-dimnesions using PCA. This can be done by first running SVD. The first three columns of SVD are the PCs as MATLAB creates S with values in descending order. The PCA projected data can then be obtained simply by projecting the data onto these PCs.</p><pre class="codeinput">load(<span class="string">'windowedSpikes.mat'</span>)

[U, S, V] = svd(data); <span class="comment">% Peforming SVD on the data</span>

reduced_data = data * V(:, 1:3); <span class="comment">% PCs are the first three columns of V</span>

figure();
scatter3(reduced_data(:, 1), reduced_data(:, 2), reduced_data(:, 3), <span class="keyword">...</span>
    <span class="string">'s'</span>, <span class="string">'filled'</span>, <span class="string">'MarkerEdgeColor'</span>,<span class="string">'k'</span>, <span class="string">'MarkerFaceColor'</span>, [0 .5 .75])
xlabel(<span class="string">'PC1'</span>)
ylabel(<span class="string">'PC2'</span>)
zlabel(<span class="string">'PC3'</span>)
title(<span class="string">'PCA of spike waveforms'</span>)
set(gca, <span class="string">'FontSize'</span>, 14, <span class="string">'LineWidth'</span>, 2)
</pre><img vspace="5" hspace="5" src="Q3b_01.png" alt=""> <p>This lower-dimensional data suggests that there are either 3 or 4 clusters present in it and hence it is likely that either 3 or 4 neurons are contribute to this data. However, to say definitively, we can run a k-mens clustering algorithm. Here a soft k-means is implemented that accounts for fuzziness in the clustering through a beta parameter in its cost function. Computing soft k-means for a range of k values, we can compute the average Euclidean distance between each datapoint and the centroid of the cluster that its assigned to. Plotting these distances gives an elbow plot that shows that the error is higher for smaller k values and decreases as a function of k. However, for higher values of k, these errors saturate out. The elbow method suggests that the smallest value of k, after which the average euclidean distance is negligible is an optimum number of clusters. From this elbow plot, we can see that K = 4 seems to be the best.</p><pre class="codeinput">K = 1:7;
avg_euc_ = zeros(length(K), 1);
<span class="keyword">for</span> kk = K
    [~, avg_euc_dist, ~] = soft_kmeans(reduced_data, kk, 1e3, 2);
    avg_euc_(kk) = avg_euc_dist;
<span class="keyword">end</span>

figure()
plot(K, avg_euc_, <span class="string">'ko-'</span>, <span class="string">'LineWidth'</span>, 1.5)
xlabel(<span class="string">'K'</span>)
ylabel(<span class="string">'Avg. Euclidean distance'</span>)
set(gca, <span class="string">'LineWidth'</span>, 2, <span class="string">'FontSize'</span>, 14)
title(<span class="string">'Elbow plot'</span>)
</pre><img vspace="5" hspace="5" src="Q3b_02.png" alt=""> <p>Computing the cluster centers, responsibilities and color-coding each data point based on the cluster that its assigned to, we get:</p><pre class="codeinput">kk = 4;
[res, avg_euc_dist, centers] = soft_kmeans(reduced_data, kk, 1e3, 2);
plot_soft_kmeans(reduced_data, res, kk)
</pre><img vspace="5" hspace="5" src="Q3b_03.png" alt=""> <p>Thus, we can see that K = 4 seems to be doing a good job at clustering this data. Hence we can say that these neural recordings most likely arose from 4 neurons.</p><pre class="codeinput">figure()
</pre><img vspace="5" hspace="5" src="Q3b_04.png" alt=""> <h2 id="9">Functions</h2><pre class="codeinput"><span class="keyword">function</span> [res, avg_euc_dist, centers] = soft_kmeans(data, K, iters, beta)
    <span class="comment">% For each iteration, the function computes the responsibility of each</span>
    <span class="comment">% datapoint to the cluster-centroid and based on these responsiblities</span>
    <span class="comment">% it then updates the centers. The process is repeated for the number</span>
    <span class="comment">% of iterations specified. Finally the average Euclidean distance is</span>
    <span class="comment">% computed for each datapoint from the centroid of the respective</span>
    <span class="comment">% cluster it is assigned to.</span>
    [~, dim] = size(data);
    centers = randi([-10, 10], K, dim);
    <span class="keyword">for</span> ii = 1:iters
        res = get_responsibilities(data, K, centers, beta);
        centers = compute_centers(data, K, res);
    <span class="keyword">end</span>

    avg_euc_dist = avg_dist(data, res, centers);
<span class="keyword">end</span>

<span class="keyword">function</span> res = get_responsibilities(data, K, cen, beta)
    <span class="comment">% computes responsibilites for data point to a given cluster defined by</span>
    <span class="comment">% its cluster centroid. The optimization function using a fuzziness</span>
    <span class="comment">% parameter beta controls the "softness" of clustering. The</span>
    <span class="comment">% responsibilities are normalized to have a unit sum.</span>
    nPoints = size(data, 1);
    res = zeros(K, nPoints);
    <span class="keyword">for</span> i = 1:K
        res(i, :) = exp(-beta * sqrt(sum(abs(data - cen(i, :)), 2)));
    <span class="keyword">end</span>
    res = res./sum(res);
<span class="keyword">end</span>

<span class="keyword">function</span> center = compute_centers(data, K, responsibility)
    <span class="comment">% Update the centers by computing an element-wise product of datapoints</span>
    <span class="comment">% to the responsibility of that datapoint to each cluster centroid.</span>
    [nPoints, dim] = size(data);
    center = zeros(K, dim);
    total_res = sum(responsibility');

    <span class="keyword">for</span> kk = 1:K
        new_cen = zeros(1, dim);
        <span class="keyword">for</span> jj = 1:nPoints
            new_cen = new_cen + data(jj, :) .* responsibility(kk, jj);
        <span class="keyword">end</span>
        center(kk, :) = new_cen./total_res(1, kk);
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">function</span> avg_euc_dist = avg_dist(data, res, centers)
    <span class="comment">% The Euclidean distance is computed between each datapoint and the</span>
    <span class="comment">% centroid of each cluster. The average Euclidean distance is then</span>
    <span class="comment">% computed as the mean of the Euclidean distances.</span>
    euc_dist = zeros(length(data), 1);
    <span class="keyword">if</span> isrow(res) <span class="comment">% for the case of K = 1</span>
        <span class="keyword">for</span> ii = 1:length(data)
            euc_dist(ii) = sqrt(sum(data(ii, :) - centers).^2);
        <span class="keyword">end</span>
    <span class="keyword">else</span>
        [~, class_selected] = max(res);
        <span class="keyword">for</span> ii = 1:length(data)
           euc_dist(ii) = sqrt(sum(data(ii, :) - centers(class_selected(ii), :)).^2);
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    avg_euc_dist = mean(euc_dist);
<span class="keyword">end</span>

<span class="keyword">function</span> plot_soft_kmeans(data, res, K)
    <span class="comment">% This function plots the data with the random color codes based on the</span>
    <span class="comment">% clusters that they are assigned to</span>
    random_colors = rand(K, 3);
    colors = res' * random_colors;
    scatter3(data(:, 1), data(:, 2), data(:, 3), [], colors, <span class="string">'s'</span>, <span class="string">'filled'</span>)
    xlabel(<span class="string">'PC1'</span>)
    ylabel(<span class="string">'PC2'</span>)
    zlabel(<span class="string">'PC3'</span>)
    title(<span class="string">'soft k-means clustering of PCA'</span>)
    set(gca, <span class="string">'FontSize'</span>, 14, <span class="string">'LineWidth'</span>, 2)
<span class="keyword">end</span>
</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2021b</a><br></p></div><!--
##### SOURCE BEGIN #####
clear; close all; clc;

%%
% The extracellular neural recordings suggested that they arose from either
% 3 or 4 neurons using the preliminary analyses done in HW2. With the new
% tools of clustering, we now have a more elegant way of running the
% analyses to determine the number of neurons that are responsible for the
% data. First, we project the higher dimensional data onto a
% lower-dimnesions using PCA. This can be done by first running SVD. The
% first three columns of SVD are the PCs as MATLAB creates S with values in
% descending order. The PCA projected data can then be obtained simply by
% projecting the data onto these PCs.
%%
load('windowedSpikes.mat')

[U, S, V] = svd(data); % Peforming SVD on the data

reduced_data = data * V(:, 1:3); % PCs are the first three columns of V

figure();
scatter3(reduced_data(:, 1), reduced_data(:, 2), reduced_data(:, 3), ...
    's', 'filled', 'MarkerEdgeColor','k', 'MarkerFaceColor', [0 .5 .75])
xlabel('PC1')
ylabel('PC2')
zlabel('PC3')
title('PCA of spike waveforms')
set(gca, 'FontSize', 14, 'LineWidth', 2)

%%
% This lower-dimensional data suggests that there are either 3 or 4
% clusters present in it and hence it is likely that either 3 or 4 neurons
% are contribute to this data. However, to say definitively, we can run a
% k-mens clustering algorithm. Here a soft k-means is implemented that
% accounts for fuzziness in the clustering through a beta parameter in its
% cost function. Computing soft k-means for a range of k values, we can
% compute the average Euclidean distance between each datapoint and the
% centroid of the cluster that its assigned to. Plotting these distances
% gives an elbow plot that shows that the error is higher for smaller k
% values and decreases as a function of k. However, for higher values of k,
% these errors saturate out. The elbow method suggests that the smallest
% value of k, after which the average euclidean distance is negligible is
% an optimum number of clusters. From this elbow plot, we can see that K =
% 4 seems to be the best.
%%
K = 1:7;
avg_euc_ = zeros(length(K), 1);
for kk = K
    [~, avg_euc_dist, ~] = soft_kmeans(reduced_data, kk, 1e3, 2);
    avg_euc_(kk) = avg_euc_dist;
end

figure()
plot(K, avg_euc_, 'ko-', 'LineWidth', 1.5)
xlabel('K')
ylabel('Avg. Euclidean distance')
set(gca, 'LineWidth', 2, 'FontSize', 14)
title('Elbow plot')

%%
% Computing the cluster centers, responsibilities and color-coding each
% data point based on the cluster that its assigned to, we get:
%%
kk = 4;
[res, avg_euc_dist, centers] = soft_kmeans(reduced_data, kk, 1e3, 2);
plot_soft_kmeans(reduced_data, res, kk)

%%
% Thus, we can see that K = 4 seems to be doing a good job at clustering
% this data. Hence we can say that these neural recordings most likely
% arose from 4 neurons.
figure()
%% Functions
function [res, avg_euc_dist, centers] = soft_kmeans(data, K, iters, beta)
    % For each iteration, the function computes the responsibility of each
    % datapoint to the cluster-centroid and based on these responsiblities
    % it then updates the centers. The process is repeated for the number
    % of iterations specified. Finally the average Euclidean distance is
    % computed for each datapoint from the centroid of the respective
    % cluster it is assigned to.
    [~, dim] = size(data);
    centers = randi([-10, 10], K, dim);
    for ii = 1:iters
        res = get_responsibilities(data, K, centers, beta);
        centers = compute_centers(data, K, res);
    end
    
    avg_euc_dist = avg_dist(data, res, centers);
end

function res = get_responsibilities(data, K, cen, beta)
    % computes responsibilites for data point to a given cluster defined by
    % its cluster centroid. The optimization function using a fuzziness
    % parameter beta controls the "softness" of clustering. The
    % responsibilities are normalized to have a unit sum.
    nPoints = size(data, 1);
    res = zeros(K, nPoints);
    for i = 1:K
        res(i, :) = exp(-beta * sqrt(sum(abs(data - cen(i, :)), 2)));
    end
    res = res./sum(res);
end

function center = compute_centers(data, K, responsibility)
    % Update the centers by computing an element-wise product of datapoints
    % to the responsibility of that datapoint to each cluster centroid.
    [nPoints, dim] = size(data);
    center = zeros(K, dim);
    total_res = sum(responsibility');
    
    for kk = 1:K
        new_cen = zeros(1, dim);
        for jj = 1:nPoints
            new_cen = new_cen + data(jj, :) .* responsibility(kk, jj);
        end
        center(kk, :) = new_cen./total_res(1, kk);
    end
end

function avg_euc_dist = avg_dist(data, res, centers)
    % The Euclidean distance is computed between each datapoint and the
    % centroid of each cluster. The average Euclidean distance is then
    % computed as the mean of the Euclidean distances.
    euc_dist = zeros(length(data), 1);
    if isrow(res) % for the case of K = 1
        for ii = 1:length(data)
            euc_dist(ii) = sqrt(sum(data(ii, :) - centers).^2);
        end
    else
        [~, class_selected] = max(res);
        for ii = 1:length(data)
           euc_dist(ii) = sqrt(sum(data(ii, :) - centers(class_selected(ii), :)).^2);
        end
    end
    avg_euc_dist = mean(euc_dist);
end

function plot_soft_kmeans(data, res, K)
    % This function plots the data with the random color codes based on the
    % clusters that they are assigned to
    random_colors = rand(K, 3);
    colors = res' * random_colors;
    scatter3(data(:, 1), data(:, 2), data(:, 3), [], colors, 's', 'filled')
    xlabel('PC1')
    ylabel('PC2')
    zlabel('PC3')
    title('soft k-means clustering of PCA')
    set(gca, 'FontSize', 14, 'LineWidth', 2)
end

##### SOURCE END #####
--></body></html>